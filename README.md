# nlp-readings
### Summaries of the papers I read for the NLP course at Asian Institute of Technology

## Assignment 1
### Token Dropping for Efficient BERT Pretraining (ACL 2022)
#### by Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou

| Problem  | To reduce the pretraining costs of computationally expensive BERT models without degrading their performance  |
| Key Related Works  | 1. ALBERT: A lite BERT for self-supervised learning of language representations (Lan et. al., 2020)  |
|   | 2. Megatron-lm: Training multi-billion parameter language models using model parallelism (Shoeybi et. al., 2019)  |
|   | 3. Large batch optimization for deep learning: Training bert in 76 minutes (You et. al., 2020)  |
| Solution  | Content Cell  |
| Result  | Content Cell  |
